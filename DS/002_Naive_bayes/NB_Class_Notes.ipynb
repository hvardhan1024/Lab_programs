{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d834f02-189c-4a1a-a079-10cb5702c044",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier\n",
    "NB classifiers are superviseds ML algorithms used for classification tasks, based on Bayes Theorm to find probabilities\n",
    "\n",
    "Bayes theorem helps to determine the conditional probability of an even based on prior knowledge and new evidence. \n",
    "\n",
    "It adjusts probabilites when new information comes in and helps make better decisoin in undertain situations. \n",
    "\n",
    "### Bayes Theorem formula \n",
    "\n",
    "for any two events A and B, then the formula for the Bayes theorem is given by:\n",
    "\n",
    "\n",
    "$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$\n",
    "\n",
    "\n",
    "Where,\n",
    "- P(A) and P(B) are the probabilities of events A and B also they are never equal to zero\n",
    "- P(A|B) is the probability of even A when event B happens\n",
    "- P(B|A) is the probability of event B when A happens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e53dd9-0f17-40c7-a704-7de48402bcae",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "- it is a simple probabilistic classifier and it has very few number of parameters which are used to build the ML model that can predict at a faster speed than other classification algorithms\n",
    "- It is a **Probabilistic** because it assumes that one feature in the model is independent of existence of another feature. In other words, each feature contributes to the predications with no realtion between each other.\n",
    "- Naive Bayes Algorithm is used in spam filtration, sentimental analysis, classifying aritcles and many more\n",
    "- It is named \"Naive\" because it assumes the presence of one featrue does not affect the other features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8389fa-f373-452d-b0fd-86293751acc5",
   "metadata": {},
   "source": [
    "### Assumptions\n",
    "\n",
    "- Feature independence: This means that we are trying to classify something, we assume that each feautre in the data is independent \n",
    "- Continuous features are normally distributed\n",
    "- Discreate features have multinomial distributions:\n",
    "- Features are equally important : all features are assumed to contribute equally to the prediction of the class lable.\n",
    "- No missing data: the data should not contain any missing values\n",
    "\n",
    "### Advantages \n",
    "- Easy to implement and computationally efficient\n",
    "- Effective in cases with large number of features\n",
    "- performs well in the presence of training data\n",
    "- It performs well in the presencec of categorical features\n",
    "- For numerical features data is assumed to come from normal distribution\n",
    "\n",
    "### Disadvatages\n",
    "- Assumes that features are independent which may not always hold in real world data.\n",
    "- can be influenced by irrelevant attributes.\n",
    "- may assign zero probability to unseen events, leading to poor generalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af30181-ad81-41ca-9f0e-ce67589d8b9d",
   "metadata": {},
   "source": [
    "### Applications of NB classifier \n",
    "- Spam email filtering\n",
    "- Text Classification\n",
    "- Medical Diagnosis\n",
    "- Credit Scoring\n",
    "- Weather Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb50f8f-7ff5-4248-97e9-fec63c0b62dc",
   "metadata": {},
   "source": [
    "### Difference between different naive bayes model \n",
    "\n",
    "\n",
    "| Aspect               | Gaussian Naive Bayes                                  | Multinomial Naive Bayes                                | Bernoulli Naive Bayes                                  |\n",
    "|----------------------|--------------------------------------------------------|--------------------------------------------------------|--------------------------------------------------------|\n",
    "| **Feature Type**      | Continuous (real valued features)                      | Discrete (count data or frequency based features)       | Binary (presence or absence of data)                   |\n",
    "| **Assumption**        | Assumes data follows a Gaussian (normal distribution)  | Assumes data follows a multinomial distribution         | Assumes data follows a Bernoulli distribution          |\n",
    "| **Common use case**   | Continuous features like height, weight, etc.          | Suitable for text classification (word counts)         | Suitable for binary classification tasks e.g., spam detection |\n",
    "| **Data Representation**| Numeric features (real-valued)                        | Count-based (discrete)                                 | Binary (0 or 1 presence/absence)                       |\n",
    "| **Mathematical Model**| Uses Gaussian distribution (mean and variance for each feature) | Uses multinomial distribution for word counts in text classification | Uses Bernoulli probability of a feature being present |\n",
    "| **Example**           | Predicting whether an email is spam based on numeric features | Predicting whether a document is spam based on word counts | Classifying a document as spam or not based on word presence |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236ae576-474c-4bd5-8d42-3b47e4e78ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
